#!/usr/bin/env python3
"""
Debug script to investigate the model selection issue.
"""

import requests
import time
import json

API_BASE_URL = "http://localhost:8000"

def test_api_connection():
    """Test if the API is accessible."""
    try:
        response = requests.get(f"{API_BASE_URL}/api/health", timeout=5)
        return response.status_code == 200
    except:
        return False

def debug_model_selection_issue():
    """Debug the model selection issue step by step."""
    print("ğŸ” Debugging Model Selection Issue")
    print("=" * 60)
    
    if not test_api_connection():
        print("âŒ Cannot connect to API server. Please start the backend first.")
        return
    
    try:
        # Step 1: Check current API configuration
        print("1. Checking current API configuration...")
        response = requests.get(f"{API_BASE_URL}/api/config/llm")
        if response.status_code == 200:
            current_config = response.json()
            current_model = current_config.get('ollama_model', 'Unknown')
            print(f"   ğŸ“‹ API reports current model: {current_model}")
        else:
            print(f"   âŒ Failed to get config: {response.status_code}")
            return
        
        # Step 2: Check available models
        print("\n2. Checking available models...")
        response = requests.get(f"{API_BASE_URL}/api/config/ollama/models")
        if response.status_code == 200:
            models_data = response.json()
            available_models = models_data.get('vision_models', [])
            print(f"   ğŸ“Š Available vision models: {available_models}")
            
            # Check if huihui_ai/gemma3 is available
            if 'huihui_ai/gemma3' in available_models:
                target_model = 'huihui_ai/gemma3'
                print(f"   âœ… Target model '{target_model}' is available")
            elif 'huihui_ai' in available_models:
                target_model = 'huihui_ai'
                print(f"   âœ… Alternative target model '{target_model}' is available")
            else:
                print("   âŒ Neither 'huihui_ai/gemma3' nor 'huihui_ai' are available")
                print("   ğŸ’¡ Available models:", available_models)
                if available_models:
                    target_model = available_models[0]
                    print(f"   ğŸ”„ Using '{target_model}' for testing")
                else:
                    print("   âŒ No vision models available")
                    return
        else:
            print(f"   âŒ Failed to get models: {response.status_code}")
            return
        
        # Step 3: Update model via API
        print(f"\n3. Updating model to '{target_model}' via API...")
        update_data = {"ollama_model": target_model}
        response = requests.put(f"{API_BASE_URL}/api/config/llm", json=update_data)
        if response.status_code == 200:
            result = response.json()
            print(f"   âœ… API update successful")
            print(f"   ğŸ“„ Response: {json.dumps(result, indent=2)}")
        else:
            print(f"   âŒ Failed to update model: {response.status_code}")
            print(f"   ğŸ“„ Error response: {response.text}")
            return
        
        # Step 4: Verify API reflects the change immediately
        print("\n4. Verifying API reflects the change...")
        time.sleep(0.5)
        response = requests.get(f"{API_BASE_URL}/api/config/llm")
        if response.status_code == 200:
            updated_config = response.json()
            new_model = updated_config.get('ollama_model', 'Unknown')
            print(f"   ğŸ“‹ API now reports model: {new_model}")
            
            if new_model == target_model:
                print(f"   âœ… API correctly shows updated model!")
            else:
                print(f"   âŒ API still shows old model. Expected: {target_model}, Got: {new_model}")
                print("   ğŸ”§ This indicates the settings.OLLAMA_MODEL is not being updated")
                return
        else:
            print(f"   âŒ Failed to verify: {response.status_code}")
            return
        
        # Step 5: Test with a simple analysis to see what model is actually used
        print("\n5. Testing actual model usage...")
        print("   ğŸ’¡ Now trigger an analysis operation (upload image or start indexing)")
        print("   ğŸ’¡ Check server logs for:")
        print(f"   ğŸ’¡   - 'LLM service initialized with model: [model_name]'")
        print(f"   ğŸ’¡   - 'ğŸ¤– Calling Ollama API with model: {target_model}'")
        print("")
        print("   ğŸ” If logs still show 'gemma3:4b', the issue is:")
        print("   ğŸ”   1. Singleton pattern not working (multiple instances)")
        print("   ğŸ”   2. LLM service was initialized before model change")
        print("   ğŸ”   3. Settings object is not the same instance")
        
        # Step 6: Additional debugging info
        print("\n6. Additional debugging steps:")
        print("   ğŸ“‹ Check server logs for:")
        print("   ğŸ“‹   - How many 'LLM service initialized' messages appear")
        print("   ğŸ“‹   - When they appear relative to model changes")
        print("   ğŸ“‹   - What model each initialization shows")
        print("")
        print("   ğŸ”§ If multiple 'LLM service initialized' messages:")
        print("   ğŸ”§   â†’ Singleton pattern is not working")
        print("   ğŸ”§   â†’ Multiple instances are being created")
        print("")
        print("   ğŸ”§ If only one initialization but wrong model:")
        print("   ğŸ”§   â†’ LLM service was created before model update")
        print("   ğŸ”§   â†’ Need to ensure dynamic property is working")
        
        return True
        
    except Exception as e:
        print(f"âŒ Debug failed with error: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_settings_object_identity():
    """Test if the settings object is consistent."""
    print("\nğŸ”¬ Testing Settings Object Identity")
    print("=" * 50)
    
    print("This test requires backend code inspection:")
    print("1. Check if 'from app.config import settings' imports the same object everywhere")
    print("2. Verify that settings.OLLAMA_MODEL is actually being modified")
    print("3. Confirm that the LLM service's self.model property reads from the same settings")
    print("")
    print("ğŸ’¡ Add debug logging to LLM service model property:")
    print("   @property")
    print("   def model(self):")
    print("       logger.info(f'ğŸ” LLM service model property called, returning: {settings.OLLAMA_MODEL}')")
    print("       return settings.OLLAMA_MODEL")

def main():
    """Main debug function."""
    print("ğŸ› Model Selection Issue Debug Tool")
    print("=" * 60)
    
    success = debug_model_selection_issue()
    test_settings_object_identity()
    
    print("\nğŸ“‹ Summary of Potential Issues:")
    print("=" * 50)
    print("1. ğŸ”„ Singleton Pattern Issues:")
    print("   - Multiple LLM service instances still being created")
    print("   - Class-level variables not working as expected")
    print("")
    print("2. âš™ï¸  Settings Object Issues:")
    print("   - Different settings instances in different modules")
    print("   - settings.OLLAMA_MODEL not actually being updated")
    print("")
    print("3. ğŸ• Timing Issues:")
    print("   - LLM service initialized before model change")
    print("   - Property not being called when expected")
    print("")
    print("4. ğŸ” Debugging Steps:")
    print("   - Add logging to model property")
    print("   - Check singleton instance creation")
    print("   - Verify settings object identity")
    print("   - Monitor initialization timing")
    
    if success:
        print("\nâœ… API level changes are working correctly")
        print("ğŸ”§ Issue is likely in LLM service singleton or property access")
    else:
        print("\nâŒ API level changes are not working")
        print("ğŸ”§ Issue is in the settings update mechanism")

if __name__ == "__main__":
    main()
