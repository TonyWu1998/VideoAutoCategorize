#!/usr/bin/env python3
"""
Script to trigger LLM service usage and test the singleton pattern.
"""

import requests
import base64
import io
from PIL import Image

API_BASE_URL = "http://localhost:8000"

def create_test_image():
    """Create a simple test image."""
    # Create a simple 100x100 red image
    img = Image.new('RGB', (100, 100), color='red')
    
    # Convert to bytes
    img_bytes = io.BytesIO()
    img.save(img_bytes, format='PNG')
    img_bytes.seek(0)
    
    return img_bytes.getvalue()

def trigger_llm_via_search():
    """Try to trigger LLM via search endpoint."""
    print("ğŸ” Attempting to trigger LLM via search...")
    
    try:
        # Try a simple search that might trigger embedding generation
        response = requests.get(f"{API_BASE_URL}/api/search/simple?q=test&limit=1")
        if response.status_code == 200:
            print("   âœ… Search request successful")
            return True
        else:
            print(f"   âŒ Search failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ Search error: {e}")
        return False

def trigger_llm_via_indexing():
    """Try to trigger LLM via indexing status check."""
    print("ğŸ”„ Attempting to trigger LLM via indexing status...")
    
    try:
        # Check indexing status - this might trigger LLM service creation
        response = requests.get(f"{API_BASE_URL}/api/index/status")
        if response.status_code == 200:
            print("   âœ… Indexing status request successful")
            return True
        else:
            print(f"   âŒ Indexing status failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"   âŒ Indexing status error: {e}")
        return False

def check_logs_for_singleton():
    """Check if singleton debug messages appeared in logs."""
    print("\nğŸ“‹ Checking for singleton debug messages...")
    
    try:
        # Read recent logs
        import subprocess
        result = subprocess.run(
            ["tail", "-n", "50", "backend/logs/app.log"],
            capture_output=True,
            text=True,
            cwd="/Users/tony/Desktop/projects/VideoAutoCategorize"
        )
        
        if result.returncode == 0:
            logs = result.stdout
            
            # Check for our debug messages
            singleton_messages = [
                "ğŸ” Creating NEW LLM service singleton instance",
                "ğŸ” Returning EXISTING LLM service singleton instance",
                "ğŸ” LLM service __init__ starting initialization",
                "ğŸ” LLM service model property accessed",
                "huihui_ai/gemma3-abliterated:4b"
            ]
            
            found_messages = []
            for message in singleton_messages:
                if message in logs:
                    found_messages.append(message)
            
            if found_messages:
                print("   âœ… Found singleton debug messages:")
                for msg in found_messages:
                    print(f"      - {msg}")
                return True
            else:
                print("   âŒ No singleton debug messages found")
                print("   ğŸ’¡ LLM service hasn't been recreated yet")
                return False
        else:
            print(f"   âŒ Failed to read logs: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"   âŒ Log check error: {e}")
        return False

def main():
    """Main function to trigger LLM and test singleton."""
    print("ğŸ§ª Trigger LLM Service Test")
    print("=" * 50)
    
    # Try different ways to trigger LLM service
    print("1. Trying to trigger LLM service...")
    
    # Method 1: Search
    search_success = trigger_llm_via_search()
    
    # Method 2: Indexing status
    indexing_success = trigger_llm_via_indexing()
    
    # Check logs
    singleton_found = check_logs_for_singleton()
    
    print("\nğŸ“‹ Results Summary:")
    print("=" * 50)
    print(f"ğŸ” Search trigger: {'âœ… Success' if search_success else 'âŒ Failed'}")
    print(f"ğŸ”„ Indexing trigger: {'âœ… Success' if indexing_success else 'âŒ Failed'}")
    print(f"ğŸ”§ Singleton messages: {'âœ… Found' if singleton_found else 'âŒ Not found'}")
    
    if not singleton_found:
        print("\nğŸ’¡ LLM service hasn't been recreated yet.")
        print("ğŸ’¡ Try one of these to force LLM service creation:")
        print("   1. Upload an image via the web UI")
        print("   2. Start an indexing operation")
        print("   3. Restart the server to clear existing instances")
        print("")
        print("ğŸ” Then check logs for:")
        print("   - 'ğŸ” Creating NEW LLM service singleton instance'")
        print("   - 'LLM service initialized with model: huihui_ai/gemma3-abliterated:4b'")
        print("   - 'ğŸ¤– Calling Ollama API with model: huihui_ai/gemma3-abliterated:4b'")
    else:
        print("\nğŸ‰ Singleton pattern appears to be working!")
        print("âœ… Check the logs to verify the model is now 'huihui_ai/gemma3-abliterated:4b'")

if __name__ == "__main__":
    main()
